{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted by: Frank Greco\n",
    "<br>\n",
    "From here: https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac\n",
    "<br>\n",
    "Posted by: aneesh joshi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def original_corpus():\n",
    "    #corpus_raw = 'He is the king . The king is royal. She is the royal queen.'\n",
    "    corpus_raw = \"All right, so we're going to need someone from all the groups to kind of summarize for us. Grace? Guys, raise your hands. So I figured okay, you guys should . All right, I need everybody on the floor, and you two groups sit right here, okay? Everybody's on the floor, criss-cross applesauce, in five. Five, four, three two... and one. Okay. I need you to stay where you are and just turn around, and I'll just have the people come talk to me, and they'll talk to you, all right? We're going to start with... let's see. We're going to start with group one, all right? People from group one, raise your hand. Okay. Will you girls come up and tell me what you guys talked about? And I'm going to listen. Our question was, select three characteristics that you think is most important for school and tell why. So we had our score card, and it said stuff. So then the first one we did was my manners, because if you're nice, that means that you're polite, and then you get more friends. All right. Next?\"\n",
    "    # convert to lower case\n",
    "    corpus_raw = corpus_raw.lower()\n",
    "    return corpus_raw\n",
    "corpus_raw=original_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "right,\n",
      "so\n",
      "we're\n",
      "going\n",
      "to\n",
      "need\n",
      "someone\n",
      "from\n",
      "all\n",
      "the\n",
      "groups\n",
      "to\n",
      "kind\n",
      "of\n",
      "summarize\n",
      "for\n",
      "us.\n",
      "grace?\n",
      "guys,\n",
      "raise\n",
      "your\n",
      "hands.\n",
      "so\n",
      "i\n",
      "figured\n",
      "okay,\n",
      "you\n",
      "guys\n",
      "should\n",
      ".\n",
      "all\n",
      "right,\n",
      "i\n",
      "need\n",
      "everybody\n",
      "on\n",
      "the\n",
      "floor,\n",
      "and\n",
      "you\n",
      "two\n",
      "groups\n",
      "sit\n",
      "right\n",
      "here,\n",
      "okay?\n",
      "everybody's\n",
      "on\n",
      "the\n",
      "floor,\n",
      "criss-cross\n",
      "applesauce,\n",
      "in\n",
      "five.\n",
      "five,\n",
      "four,\n",
      "three\n",
      "two...\n",
      "and\n",
      "one.\n",
      "okay.\n",
      "i\n",
      "need\n",
      "you\n",
      "to\n",
      "stay\n",
      "where\n",
      "you\n",
      "are\n",
      "and\n",
      "just\n",
      "turn\n",
      "around,\n",
      "and\n",
      "i'll\n",
      "just\n",
      "have\n",
      "the\n",
      "people\n",
      "come\n",
      "talk\n",
      "to\n",
      "me,\n",
      "and\n",
      "they'll\n",
      "talk\n",
      "to\n",
      "you,\n",
      "all\n",
      "right?\n",
      "we're\n",
      "going\n",
      "to\n",
      "start\n",
      "with...\n",
      "let's\n",
      "see.\n",
      "we're\n",
      "going\n",
      "to\n",
      "start\n",
      "with\n",
      "group\n",
      "one,\n",
      "all\n",
      "right?\n",
      "people\n",
      "from\n",
      "group\n",
      "one,\n",
      "raise\n",
      "your\n",
      "hand.\n",
      "okay.\n",
      "will\n",
      "you\n",
      "girls\n",
      "come\n",
      "up\n",
      "and\n",
      "tell\n",
      "me\n",
      "what\n",
      "you\n",
      "guys\n",
      "talked\n",
      "about?\n",
      "and\n",
      "i'm\n",
      "going\n",
      "to\n",
      "listen.\n",
      "our\n",
      "question\n",
      "was,\n",
      "select\n",
      "three\n",
      "characteristics\n",
      "that\n",
      "you\n",
      "think\n",
      "is\n",
      "most\n",
      "important\n",
      "for\n",
      "school\n",
      "and\n",
      "tell\n",
      "why.\n",
      "so\n",
      "we\n",
      "had\n",
      "our\n",
      "score\n",
      "card,\n",
      "and\n",
      "it\n",
      "said\n",
      "stuff.\n",
      "so\n",
      "then\n",
      "the\n",
      "first\n",
      "one\n",
      "we\n",
      "did\n",
      "was\n",
      "my\n",
      "manners,\n",
      "because\n",
      "if\n",
      "you're\n",
      "nice,\n",
      "that\n",
      "means\n",
      "that\n",
      "you're\n",
      "polite,\n",
      "and\n",
      "then\n",
      "you\n",
      "get\n",
      "more\n",
      "friends.\n",
      "all\n",
      "right.\n",
      "next?\n",
      "vocab_size 103\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for word in corpus_raw.split():\n",
    "    print word\n",
    "    if word != '.':        # because we don't want to treat . as a word\n",
    "        word=word.replace(\",\", \"\")\n",
    "        word=word.replace(\".\", \"\")\n",
    "        word=word.replace(\"?\", \"\")\n",
    "        word=word.replace(\"!\", \"\")  \n",
    "        words.append(word)\n",
    "\n",
    "words = set(words) # so that all duplicate words are removed\n",
    "\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "\n",
    "vocab_size = len(words) # gives the total number of unique words\n",
    "\n",
    "print \"vocab_size\", vocab_size\n",
    "\n",
    "for i,word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "print(word2int['us'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four\n"
     ]
    }
   ],
   "source": [
    "print(int2word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# raw sentences is a list of sentences.\n",
    "raw_sentences = corpus_raw.split('.')\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "WINDOW_SIZE = 2\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "            word=word.replace(\",\", \"\")\n",
    "            word=word.replace(\".\", \"\")\n",
    "            word=word.replace(\"?\", \"\")\n",
    "            word=word.replace(\"!\", \"\")\n",
    "            \n",
    "            nb_word=nb_word.replace(\",\", \"\")\n",
    "            nb_word=nb_word.replace(\".\", \"\")\n",
    "            nb_word=nb_word.replace(\"?\", \"\")\n",
    "            nb_word=nb_word.replace(\"!\", \"\")\n",
    "            \n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])\n",
    "            else:\n",
    "                print word, nb_word\n",
    "            #data.append([word, nb_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to convert numbers to one hot vectors\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [] # input word\n",
    "y_train = [] # output word\n",
    "\n",
    "for data_word in data:\n",
    "    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n",
    "\n",
    "# convert them to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making placeholders for x_train and y_train\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 5 # you can choose your own number\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) #bias\n",
    "\n",
    "hidden_representation = tf.add(tf.matmul(x,W1), b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init) #make sure you do this!\n",
    "\n",
    "# define the loss function:\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n",
    "\n",
    "# define the training step:\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
    "\n",
    "n_iters = 10000\n",
    "\n",
    "# train for n_iter iterations\n",
    "\n",
    "for _ in range(n_iters):\n",
    "    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n",
    "\n",
    "    print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(W1))\n",
    "print('----------')\n",
    "print(sess.run(b1))\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = sess.run(W1 + b1)\n",
    "\n",
    "\n",
    "# if you work it out, you will see that it has the same effect as running the node hidden representation\n",
    "\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors[ word2int['us'] ])\n",
    "\n",
    "# say here word2int['queen'] is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
    "\n",
    "def find_closest(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity\n",
    "    min_index = -1\n",
    "\n",
    "    query_vector = vectors[word_index]\n",
    "\n",
    "    for index, vector in enumerate(vectors):\n",
    "\n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
    "\n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index\n",
    "\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int2word[find_closest(word2int['us'], vectors)])\n",
    "print(int2word[find_closest(word2int['right'], vectors)])\n",
    "print(int2word[find_closest(word2int['criss-cross'], vectors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "vectors = model.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "normalizer = preprocessing.Normalizer()\n",
    "vectors =  normalizer.fit_transform(vectors, 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for word in words:\n",
    "    print(word, vectors[word2int[word]][1])\n",
    "    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2-venv",
   "language": "python",
   "name": "py2-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
