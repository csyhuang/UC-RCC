{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted by: Frank Greco\n",
    "<br>\n",
    "From here: https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac\n",
    "<br>\n",
    "Posted by: aneesh joshi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "indir: .\n",
      "len raw_transcript: 142\n",
      "vocab_size: 398\n",
      "word2int['president']: 104\n",
      "int2word[2]: hilda\n",
      "len(sentences): 522\n",
      "sentences[0:20]: ['okay', ' i can see two  in here', 'can you read the article first', ' please', 'yes', 'loudly', 'the president of the republic should be an egyptian born to egyptian parents and enjoy salient political rights', ' his age must not be less than 40 gregorian years', 'mm-hmm', 'i agree with the president', ' um', 'mm-hmm', 'he should be egyptian born to egyptian parents', ' because it will have', ' make him have well-rounded perspectives', ' and it may lead him to having interests to other countries', ' and so', 'it would make him have interest', ' or will not make him', ' prevent him']\n"
     ]
    }
   ],
   "source": [
    "from transcript_parser_v1 import ParsedTranscript\n",
    "pt=ParsedTranscript()\n",
    "\n",
    "pt.read_file('./Copy of 001 ENLEAYA001.srt.txt')\n",
    "\n",
    "pt.print_parms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "sentences=pt.get_tokenized_sentences()\n",
    "\n",
    "WINDOW_SIZE = 2\n",
    "\n",
    "for sentence in sentences:\n",
    "   \n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "            word=word.replace(\",\", \"\")\n",
    "            word=word.replace(\".\", \"\")\n",
    "            word=word.replace(\"?\", \"\")\n",
    "            word=word.replace(\"!\", \"\")\n",
    "            \n",
    "            nb_word=nb_word.replace(\",\", \"\")\n",
    "            nb_word=nb_word.replace(\".\", \"\")\n",
    "            nb_word=nb_word.replace(\"?\", \"\")\n",
    "            nb_word=nb_word.replace(\"!\", \"\")\n",
    "            \n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])\n",
    "            #else:\n",
    "                #print word, nb_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'can'], ['i', 'see'], ['can', 'i'], ['can', 'see'], ['can', 'two'], ['see', 'i'], ['see', 'can'], ['see', 'two'], ['see', 'in'], ['two', 'can'], ['two', 'see'], ['two', 'in'], ['two', 'here'], ['in', 'see'], ['in', 'two'], ['in', 'here'], ['here', 'two'], ['here', 'in'], ['can', 'you'], ['can', 'read']]\n",
      "5498\n"
     ]
    }
   ],
   "source": [
    "print(data[0:20])\n",
    "print len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to convert numbers to one hot vectors\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = [] # input word\n",
    "y_train = [] # output word\n",
    "\n",
    "for data_word in data:\n",
    "    x_train.append(to_one_hot(pt.get_word2int(data_word[0]), pt.vocab_size))\n",
    "    y_train.append(to_one_hot(pt.get_word2int(data_word[1]), pt.vocab_size))\n",
    "\n",
    "# convert them to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((5498, 398), (5498, 398))\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making placeholders for x_train and y_train\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, pt.vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, pt.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 5 # you can choose your own number\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([pt.vocab_size, EMBEDDING_DIM]))\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) #bias\n",
    "\n",
    "hidden_representation = tf.add(tf.matmul(x,W1), b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, pt.vocab_size]))\n",
    "\n",
    "b2 = tf.Variable(tf.random_normal([pt.vocab_size]))\n",
    "\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5498\n"
     ]
    }
   ],
   "source": [
    "print (len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('loss is : ', 11.296039)\n",
      "('loss is : ', 10.908987)\n",
      "('loss is : ', 10.588351)\n",
      "('loss is : ', 10.315809)\n",
      "('loss is : ', 10.080478)\n",
      "('loss is : ', 9.8753233)\n",
      "('loss is : ', 9.6954775)\n",
      "('loss is : ', 9.5373354)\n",
      "('loss is : ', 9.3980789)\n",
      "('loss is : ', 9.275423)\n",
      "('loss is : ', 9.1674156)\n",
      "('loss is : ', 9.0723457)\n",
      "('loss is : ', 8.988718)\n",
      "('loss is : ', 8.9151545)\n",
      "('loss is : ', 8.8504591)\n",
      "('loss is : ', 8.7935247)\n",
      "('loss is : ', 8.7433701)\n",
      "('loss is : ', 8.6991272)\n",
      "('loss is : ', 8.6600227)\n",
      "('loss is : ', 8.6253605)\n",
      "('loss is : ', 8.5945473)\n",
      "('loss is : ', 8.5670528)\n",
      "('loss is : ', 8.5424242)\n",
      "('loss is : ', 8.5202703)\n",
      "('loss is : ', 8.5002327)\n",
      "('loss is : ', 8.4820137)\n",
      "('loss is : ', 8.4653721)\n",
      "('loss is : ', 8.4500761)\n",
      "('loss is : ', 8.4359465)\n",
      "('loss is : ', 8.4228153)\n",
      "('loss is : ', 8.4105501)\n",
      "('loss is : ', 8.399024)\n",
      "('loss is : ', 8.3881407)\n",
      "('loss is : ', 8.3778095)\n",
      "('loss is : ', 8.3679581)\n",
      "('loss is : ', 8.3585291)\n",
      "('loss is : ', 8.3494625)\n",
      "('loss is : ', 8.3407183)\n",
      "('loss is : ', 8.3322439)\n",
      "('loss is : ', 8.324028)\n",
      "('loss is : ', 8.3160181)\n",
      "('loss is : ', 8.3082075)\n",
      "('loss is : ', 8.3005543)\n",
      "('loss is : ', 8.2930708)\n",
      "('loss is : ', 8.2857132)\n",
      "('loss is : ', 8.2784767)\n",
      "('loss is : ', 8.2713575)\n",
      "('loss is : ', 8.2643347)\n",
      "('loss is : ', 8.25741)\n",
      "('loss is : ', 8.2505732)\n",
      "('loss is : ', 8.2438097)\n",
      "('loss is : ', 8.2371178)\n",
      "('loss is : ', 8.2304993)\n",
      "('loss is : ', 8.2239513)\n",
      "('loss is : ', 8.2174511)\n",
      "('loss is : ', 8.2110167)\n",
      "('loss is : ', 8.2046309)\n",
      "('loss is : ', 8.1982927)\n",
      "('loss is : ', 8.1920176)\n",
      "('loss is : ', 8.1857862)\n",
      "('loss is : ', 8.179595)\n",
      "('loss is : ', 8.1734486)\n",
      "('loss is : ', 8.167347)\n",
      "('loss is : ', 8.1612902)\n",
      "('loss is : ', 8.1552763)\n",
      "('loss is : ', 8.1492872)\n",
      "('loss is : ', 8.1433439)\n",
      "('loss is : ', 8.137434)\n",
      "('loss is : ', 8.1315632)\n",
      "('loss is : ', 8.1257315)\n",
      "('loss is : ', 8.1199274)\n",
      "('loss is : ', 8.1141567)\n",
      "('loss is : ', 8.108429)\n",
      "('loss is : ', 8.1027288)\n",
      "('loss is : ', 8.0970612)\n",
      "('loss is : ', 8.0914249)\n",
      "('loss is : ', 8.0858154)\n",
      "('loss is : ', 8.0802402)\n",
      "('loss is : ', 8.0746908)\n",
      "('loss is : ', 8.0691757)\n",
      "('loss is : ', 8.0636969)\n",
      "('loss is : ', 8.0582323)\n",
      "('loss is : ', 8.052803)\n",
      "('loss is : ', 8.0473986)\n",
      "('loss is : ', 8.0420227)\n",
      "('loss is : ', 8.0366783)\n",
      "('loss is : ', 8.0313616)\n",
      "('loss is : ', 8.0260658)\n",
      "('loss is : ', 8.0208006)\n",
      "('loss is : ', 8.0155592)\n",
      "('loss is : ', 8.0103378)\n",
      "('loss is : ', 8.0051527)\n",
      "('loss is : ', 7.9999795)\n",
      "('loss is : ', 7.9948435)\n",
      "('loss is : ', 7.9897285)\n",
      "('loss is : ', 7.9846349)\n",
      "('loss is : ', 7.9795737)\n",
      "('loss is : ', 7.9745226)\n",
      "('loss is : ', 7.9695015)\n",
      "('loss is : ', 7.9645033)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init) #make sure you do this!\n",
    "\n",
    "# define the loss function:\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n",
    "\n",
    "# define the training step:\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
    "\n",
    "#n_iters = 10000\n",
    "n_iters = 100\n",
    "\n",
    "# train for n_iter iterations\n",
    "\n",
    "for _ in range(n_iters):\n",
    "    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n",
    "\n",
    "    print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.87715632  0.8764019  -0.60945106 -0.87721205 -0.43123102]\n",
      " [-1.09400022  0.94166714 -0.68897462 -0.55796325 -0.4975971 ]\n",
      " [ 0.51123565  0.17174356 -0.15058221 -1.16246557  0.47786659]\n",
      " ..., \n",
      " [ 1.29574454 -0.37181789 -0.26761407  1.85245109  0.14402483]\n",
      " [-2.05451727 -0.12604998 -1.48126268 -0.01838916 -2.06275964]\n",
      " [ 0.36526442 -0.57920974  0.085523    0.96263981  0.09425145]]\n",
      "----------\n",
      "[ 0.11640421 -0.06130994  0.27347893 -0.04166865 -0.07541812]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(W1))\n",
    "print('----------')\n",
    "print(sess.run(b1))\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99356055  0.81509197 -0.33597213 -0.9188807  -0.50664914]\n",
      " [-0.97759604  0.88035721 -0.41549569 -0.59963191 -0.57301521]\n",
      " [ 0.62763989  0.11043362  0.12289672 -1.20413423  0.40244848]\n",
      " ..., \n",
      " [ 1.41214871 -0.43312782  0.00586486  1.81078243  0.06860671]\n",
      " [-1.93811309 -0.18735991 -1.2077837  -0.06005781 -2.13817787]\n",
      " [ 0.48166862 -0.64051968  0.35900193  0.92097116  0.01883333]]\n"
     ]
    }
   ],
   "source": [
    "vectors = sess.run(W1 + b1)\n",
    "\n",
    "\n",
    "# if you work it out, you will see that it has the same effect as running the node hidden representation\n",
    "\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
    "\n",
    "def find_closest(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity\n",
    "    min_index = -1\n",
    "\n",
    "    query_vector = vectors[word_index]\n",
    "\n",
    "    for index, vector in enumerate(vectors):\n",
    "\n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
    "\n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index\n",
    "\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting\n",
      "egyptians\n",
      "presidency\n"
     ]
    }
   ],
   "source": [
    "print(pt.int2word[find_closest(pt.word2int['us'], vectors)])\n",
    "print(pt.int2word[find_closest(pt.word2int['right'], vectors)])\n",
    "print(pt.int2word[find_closest(pt.word2int['president'], vectors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "vectors = model.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "normalizer = preprocessing.Normalizer()\n",
    "vectors =  normalizer.fit_transform(vectors, 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('all', -0.12438477)\n",
      "('focus', 0.66942066)\n",
      "('hilda', 0.91116482)\n",
      "('four', 0.34461865)\n",
      "('go', -0.28501973)\n",
      "('children', -0.40758833)\n",
      "(\"35's\", -0.79661018)\n",
      "('jessie', 0.87771356)\n",
      "('supposedly', -0.80337721)\n",
      "('young', -0.9663946)\n",
      "('eighty-four', 0.087191246)\n",
      "('to', 0.27188632)\n",
      "('dislike', 0.99195039)\n",
      "('egyptian', 0.35962108)\n",
      "('very', -0.69891429)\n",
      "('results', 0.42673427)\n",
      "('decide', 0.72888273)\n",
      "('telling', 0.56788218)\n",
      "('bringing', 0.52488643)\n",
      "('governments', 0.26675659)\n",
      "('re-elect', -0.41743487)\n",
      "('dictatorship', 0.98791397)\n",
      "('did', 0.33337587)\n",
      "('notes', 0.30272898)\n",
      "('leave', 0.70647442)\n",
      "('race', 0.70040816)\n",
      "('enjoy', 0.11023678)\n",
      "('prevent', 0.27393594)\n",
      "('revolution', 0.94122547)\n",
      "('says', -0.96909827)\n",
      "('tired', 0.9686085)\n",
      "('second', -0.88264364)\n",
      "('further', -0.83350724)\n",
      "('even', 0.99349165)\n",
      "('what', -0.41230702)\n",
      "('constitution', 0.28648877)\n",
      "('specification', 0.36773896)\n",
      "('here', -0.73875397)\n",
      "('active', -0.85964751)\n",
      "('luis', 0.98406732)\n",
      "('change', -0.98052013)\n",
      "('wait', -0.13734238)\n",
      "('great', 0.38927928)\n",
      "('changed', 0.97562665)\n",
      "('experience', -0.94404364)\n",
      "('opinion', 0.288858)\n",
      "(\"i'm\", 0.99556512)\n",
      "('makes', 0.41015962)\n",
      "(\"country's\", -0.27625561)\n",
      "('elect', -0.0098337559)\n",
      "('egypt', -0.69094646)\n",
      "('africa', 0.40117639)\n",
      "('brought', 0.56288809)\n",
      "('use', -0.15058388)\n",
      "('from', -0.75800866)\n",
      "('would', 0.44618696)\n",
      "('two', -0.99409312)\n",
      "('next', -0.40356556)\n",
      "('going', -0.31427184)\n",
      "('type', -0.50086421)\n",
      "('more', 0.84740072)\n",
      "('re-elected', 0.84652287)\n",
      "('brings', -0.295724)\n",
      "('wars', -0.9997595)\n",
      "('excuse', 0.68421674)\n",
      "('basically', -0.99957353)\n",
      "('excellent', 0.25511628)\n",
      "('sorry', -0.61954767)\n",
      "('must', 0.69156682)\n",
      "('me', 0.80771106)\n",
      "('rights', -0.023131371)\n",
      "('this', 0.87379766)\n",
      "('getting', 0.91655689)\n",
      "('can', 0.43943894)\n",
      "('thirty-five', 0.7699526)\n",
      "('learn', -0.44919559)\n",
      "('following', 0.051395297)\n",
      "('making', -0.90561378)\n",
      "('yemen', -0.22089405)\n",
      "('my', -0.54644054)\n",
      "('example', 0.326092)\n",
      "('claim', -0.51513875)\n",
      "('give', 0.24561796)\n",
      "('process', 0.99709815)\n",
      "(\"didn't\", 0.46928889)\n",
      "('states', 0.99999887)\n",
      "('heard', -0.13332863)\n",
      "('something', 0.29540646)\n",
      "('want', 0.87741047)\n",
      "('guarantee', -0.39406422)\n",
      "('democracy', -0.85993159)\n",
      "('travel', 0.52808684)\n",
      "('six', -0.1322886)\n",
      "('stayed', 0.82250297)\n",
      "('how', -0.75820655)\n",
      "('responsibilities', -0.2657966)\n",
      "('economy', -0.38431382)\n",
      "('staying', -0.30193406)\n",
      "('okay', -0.89116687)\n",
      "('transforming', 0.8700732)\n",
      "('after', -0.99883884)\n",
      "('wrong', 0.99661958)\n",
      "('mad', -0.80711657)\n",
      "('date', 0.96022791)\n",
      "('president', 0.59650952)\n",
      "('types', 0.94318444)\n",
      "('a', -0.9907636)\n",
      "('remember', 0.83259988)\n",
      "('maybe', -0.8394751)\n",
      "('responsibility', 0.70842832)\n",
      "('talk', 0.4517884)\n",
      "(\"that's\", 0.93068844)\n",
      "('help', -0.8288551)\n",
      "('office', 0.87629902)\n",
      "(\"don't\", -0.9650299)\n",
      "('over', 0.54925197)\n",
      "('move', -0.99966121)\n",
      "('years', -0.9569664)\n",
      "('stability', 0.37015712)\n",
      "(\"won't\", 0.99902737)\n",
      "('its', 0.99948829)\n",
      "('20', -0.090962611)\n",
      "('thank', -0.9497422)\n",
      "(\"he's\", 0.64954853)\n",
      "('better', -0.63736576)\n",
      "('might', -0.98588419)\n",
      "('then', -0.99722946)\n",
      "('them', -0.79989898)\n",
      "('good', 0.24203356)\n",
      "('break', -0.229284)\n",
      "('they', -0.99728757)\n",
      "('not', 0.026017936)\n",
      "('now', -0.45522115)\n",
      "('dicta--', -0.19524586)\n",
      "('term', -0.40515602)\n",
      "('name', 0.57289445)\n",
      "('40s', 0.74217582)\n",
      "('went', -0.56972373)\n",
      "('mean', -0.56254768)\n",
      "('doing', 0.64701295)\n",
      "('yeah', -0.97533256)\n",
      "(\"parent's\", 0.32663709)\n",
      "('connect', 0.37097397)\n",
      "('70%', 0.24680392)\n",
      "('81', 0.97967041)\n",
      "('really', -0.93329507)\n",
      "('may', -0.71270424)\n",
      "(\"they'll\", -0.91530573)\n",
      "('theory', 0.46261704)\n",
      "('advantages', -0.73020059)\n",
      "('egyptians', -0.98011637)\n",
      "('put', -0.090612374)\n",
      "('wanted', 0.65254807)\n",
      "('could', 0.23050885)\n",
      "('computers', -0.88589442)\n",
      "('keep', -0.39683059)\n",
      "('thing', 0.99020731)\n",
      "('think', 0.89684415)\n",
      "('first', -0.52919859)\n",
      "(\"i'll\", -0.39154434)\n",
      "('already', -0.74440682)\n",
      "('number', -0.081379093)\n",
      "('one', 0.72864139)\n",
      "('specifically', -0.58912718)\n",
      "('another', 0.60434777)\n",
      "('open', 0.88726223)\n",
      "('legislative', -0.15799806)\n",
      "(\"you're\", 0.69759256)\n",
      "('little', -0.26731542)\n",
      "('salient', -0.95473826)\n",
      "('least', 0.14134902)\n",
      "('their', 0.6540482)\n",
      "('too', -0.15592003)\n",
      "('statement', 0.92282128)\n",
      "('listen', -0.86672068)\n",
      "('interests', -0.56317747)\n",
      "('jesus', 0.21032582)\n",
      "('took', 0.057682384)\n",
      "('part', -0.53619134)\n",
      "('than', 0.63690561)\n",
      "('population', 0.99834442)\n",
      "('12', 0.47607249)\n",
      "('culture', 0.80851007)\n",
      "('youth', 0.55185109)\n",
      "('were', 0.68802714)\n",
      "('result', -0.87431192)\n",
      "('and', 0.99359876)\n",
      "('teenagers', 0.78492194)\n",
      "('say', 0.71109396)\n",
      "('have', -0.89637506)\n",
      "('need', -0.34241503)\n",
      "('any', 0.83787382)\n",
      "('responsible', 0.95975304)\n",
      "('for', 0.97103792)\n",
      "('majority', 0.97298491)\n",
      "('concerns', 0.018330589)\n",
      "('take', -0.65207863)\n",
      "('which', -0.94236833)\n",
      "('so', -0.7846238)\n",
      "('sure', 0.95671356)\n",
      "('shall', 0.045508258)\n",
      "('who', -0.92578363)\n",
      "('77', -0.82092732)\n",
      "('most', -0.75196606)\n",
      "('75', 0.96739262)\n",
      "('plenty', -0.28628522)\n",
      "('70', 0.55333912)\n",
      "('america', 0.04068524)\n",
      "('why', 0.28905708)\n",
      "(\"you'll\", -0.80222762)\n",
      "('yessenia', -0.17023356)\n",
      "('laws', 0.74320686)\n",
      "('well-rounded', -0.2211531)\n",
      "('show', -0.82249182)\n",
      "('chances', -0.94767386)\n",
      "('mm-hm', 0.23225042)\n",
      "('mubarak', 0.15829305)\n",
      "('explain', 0.99178398)\n",
      "('enough', -0.02362675)\n",
      "('should', -0.54756629)\n",
      "(\"there's\", -0.83385056)\n",
      "('pretty', 0.51154321)\n",
      "('do', 0.3641265)\n",
      "('his', 0.82827544)\n",
      "('get', 0.36363915)\n",
      "('cannot', 0.23402673)\n",
      "('him', 0.27055717)\n",
      "('countries', 0.18712914)\n",
      "('emi', 0.83450794)\n",
      "('stuff', -0.35987115)\n",
      "('through', 0.24356332)\n",
      "('where', -0.90477538)\n",
      "('art', 0.74940503)\n",
      "('connections', -0.66387624)\n",
      "('elected', -0.92894131)\n",
      "('see', -0.37660944)\n",
      "('are', -0.99743527)\n",
      "('close', -0.14198273)\n",
      "('concern', -0.71460927)\n",
      "('said', 0.79189748)\n",
      "('please', 0.97503507)\n",
      "('behind', -0.9926067)\n",
      "('difficulties', -0.99196643)\n",
      "('between', 0.99927533)\n",
      "('teens', -0.38089481)\n",
      "('we', 0.50344807)\n",
      "('terms', 0.55560756)\n",
      "('parent', 0.54130214)\n",
      "('job', -0.58786881)\n",
      "('gregorian', 0.55620658)\n",
      "('article', -0.96440953)\n",
      "('come', -0.89553732)\n",
      "('both', -0.91778255)\n",
      "('libya', -0.22381818)\n",
      "('country', -0.34543872)\n",
      "('against', 0.97784609)\n",
      "('connection', -0.15427338)\n",
      "('loudly', -0.19781516)\n",
      "('compromising', 0.77354085)\n",
      "('point', -0.18416598)\n",
      "(\"he'd\", 0.78334177)\n",
      "('perspectives', 0.74680001)\n",
      "(\"it's\", 0.7247327)\n",
      "('political', -0.12921099)\n",
      "('been', 0.086213857)\n",
      "('much', 0.86502767)\n",
      "('interest', 0.98740488)\n",
      "('direction', 0.80940938)\n",
      "('educated', -0.95828599)\n",
      "(\"what's\", 0.97946805)\n",
      "('else', 0.18301591)\n",
      "('understand', -0.27296352)\n",
      "('child', 0.89978111)\n",
      "('has', 0.55777496)\n",
      "('presidency', 0.56504792)\n",
      "('also', 0.57484984)\n",
      "('will', -0.73635179)\n",
      "('many', -0.063830227)\n",
      "('mistake', 0.60894239)\n",
      "('30', 0.20074511)\n",
      "('is', 0.86863768)\n",
      "('it', 0.31701851)\n",
      "('middle', -0.89365304)\n",
      "('35', 0.97396249)\n",
      "('in', -0.97651851)\n",
      "('technology', -0.99740243)\n",
      "('if', -0.90269732)\n",
      "('different', -0.65236777)\n",
      "('things', -0.99693668)\n",
      "('make', -0.51178533)\n",
      "('same', 0.48162502)\n",
      "('rules', -0.94757903)\n",
      "('parts', 0.9126516)\n",
      "('used', 0.97589481)\n",
      "('disadvantages', 0.99705946)\n",
      "('disagree', -0.60044152)\n",
      "('lower', 0.99887121)\n",
      "('older', 0.3634356)\n",
      "('i', 0.30185711)\n",
      "('changes', 0.28111356)\n",
      "('well', -0.91237575)\n",
      "('anybody', -0.99978948)\n",
      "('80%', 0.96906471)\n",
      "('without', -0.27819914)\n",
      "('gaddafi', 0.58382279)\n",
      "('the', 0.80362236)\n",
      "('opinions', 0.43172729)\n",
      "('just', 0.077994332)\n",
      "('less', -0.9491241)\n",
      "('yes', -0.01461836)\n",
      "('announcement', -0.9923656)\n",
      "('mm-hmm', 0.75104839)\n",
      "('parents', -0.90989178)\n",
      "('teenager', 0.26820117)\n",
      "('east', 0.71719176)\n",
      "('government', 0.99599552)\n",
      "('read', 0.035502058)\n",
      "('disagreement', -0.17800799)\n",
      "('cultural', 0.82634169)\n",
      "('know', -0.84450227)\n",
      "('background', 0.35888183)\n",
      "('bit', -0.91360301)\n",
      "('like', -0.59070253)\n",
      "('specific', 0.94280678)\n",
      "('because', -0.91252339)\n",
      "('old', 0.088435113)\n",
      "('people', 0.077083893)\n",
      "('some', -0.4512518)\n",
      "('back', -0.2511965)\n",
      "('born', 0.41344723)\n",
      "('lead', -0.71252531)\n",
      "('everything', -0.93321007)\n",
      "('does', 0.057106942)\n",
      "('be', 0.39280891)\n",
      "('power', 0.12520346)\n",
      "('remembered', -0.7893163)\n",
      "('communicate', 0.85708338)\n",
      "('dictators', -0.58557826)\n",
      "('pressure', -0.3083075)\n",
      "('by', 0.19469799)\n",
      "('on', 0.7240268)\n",
      "('about', -0.28104115)\n",
      "('oh', 0.65557176)\n",
      "('of', 0.37213022)\n",
      "('or', 0.6545887)\n",
      "('hosni', 0.72928005)\n",
      "('into', 0.40592584)\n",
      "('successive', -0.21424621)\n",
      "('down', 0.53285557)\n",
      "('right', -0.96397829)\n",
      "('your', -0.51373279)\n",
      "('40-year-old', 0.57250077)\n",
      "('her', -0.47469842)\n",
      "('there', 0.38589859)\n",
      "('long', 0.67583954)\n",
      "('lot', 0.29470515)\n",
      "('was', -0.71080917)\n",
      "(\"i've\", 0.75149822)\n",
      "('becoming', -0.57300544)\n",
      "('but', -0.91183394)\n",
      "('with', 0.96796966)\n",
      "('he', 0.51399541)\n",
      "(\"they're\", 0.049347863)\n",
      "('made', -0.87275857)\n",
      "('places', 0.06862282)\n",
      "('protesting', -0.3576842)\n",
      "('up', 0.74937129)\n",
      "('us', 0.91648239)\n",
      "('um', 0.27002096)\n",
      "('uh', 0.54127288)\n",
      "('mature', 0.85848427)\n",
      "('agree', -0.085492417)\n",
      "('adults', 0.67705947)\n",
      "('certain', 0.40416896)\n",
      "('an', 0.89574873)\n",
      "('as', -0.75608885)\n",
      "('at', 0.75858146)\n",
      "('work', 0.86647087)\n",
      "('again', -0.62039834)\n",
      "('no', -0.21877581)\n",
      "('when', 0.9999758)\n",
      "('43', -0.99151808)\n",
      "('40', -0.98912549)\n",
      "('successful', 0.19438519)\n",
      "('that', 0.45052785)\n",
      "('other', -0.98167992)\n",
      "('republic', -0.74535394)\n",
      "('you', 0.81660146)\n",
      "('stay', -0.76184946)\n",
      "(\"let's\", 0.91878164)\n",
      "('important', -0.16839822)\n",
      "('younger', 0.96801358)\n",
      "('longer', -0.99844831)\n",
      "('age', -0.98989606)\n",
      "('time', -0.46948898)\n",
      "('serious', -0.99999988)\n",
      "('starting', 0.62600386)\n",
      "('having', -0.95500427)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for word in pt.words:\n",
    "    print(word, vectors[pt.word2int[word]][1])\n",
    "    ax.annotate(word, (vectors[pt.word2int[word]][0],vectors[pt.word2int[word]][1] ))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2-venv",
   "language": "python",
   "name": "py2-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
